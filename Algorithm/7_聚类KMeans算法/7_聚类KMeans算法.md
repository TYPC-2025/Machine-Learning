# 一、聚类算法分析

## 1.概念

- 概念：根据样本之间的相似性，将样本划分到不同的类别中；不同的相似度的计算方法，会得到不同的聚类结果，常见的相似度计算方法有欧氏距离法（无监督算法）
- 聚类算法的目的是在没有先验知识的情况下，自动发现数据集中的内在结构和模式

## 2.聚类算法分类

### （1）根据聚类颗粒度分类

- 个数比较多的，细聚类；个数比较多的，粗聚类

### （2）根据实现方法分类

- K-means：按照质心分类
- 层次聚类：对数据进行逐层划分，直到达到聚类的类别个数
- DBSCAN聚类是一种基于密度的聚类算法
- 谱聚类是一种基于图论的聚类算法

## 3.聚类算法API

```python
sklearn.cluster.KMeans(n_clusters=8)
```

- 参数`n_clusters`：开始的聚类中心的数量，整型，默认值为8

- 方法：`estimator.fit(x);`

  `estimator.predict(x);`

  `estimator.fit_predict(x)`：计算聚类中心并预测每个样本属于哪个类别，相当于先调用`fit(x)`，然后再调用`predict(x)`

评估

```python
from sklearn.metrics import calinski_harabasz_score

print(calinski_harabasz_score(x, y_pre))最终结果是越小越好
```

## 4.KMeans算法实现流程

1. 事先确定常数K，常数K意味着最终的聚类类别数
2. 随机选择K个样本点作为初始聚类中心
3. 计算每个样本到K个中心的距离，选择最近的聚类中心点作为标记类别
4. 根据每个类别中的样本点，重新计算出新的聚类中心点（平均值），如果计算出的新中心点与原中心点一样则停止聚类，否则重新进行第二步过程，直至聚类中心不再变化

# 二、聚类评估指标

- 不能分为训练集和测试集，则得到K值，就不能用网格交叉验证搜索

## 1.误差平方和SSE

- 公式：$SSE=\sum_{i=1}^k\sum_{p \in C_i}|p-m_i|^2$
  - $C_i$表示簇
  - $k$表示聚类中心的个数
  - $p$表示某个簇内的样本
  - $m$表示质心点
- SSE越小（用同一个数据集去做），表示数据点越接近它们的中心，聚类效果越好

## 2.肘方法——K值确定

- 肘方法通过SSE确定N__clusters的值
  - 对于n个点的数据集，迭代计算k from 1 to n, 每次聚类完成后计算SSE
  - SSE是会逐渐变小的，因为每个点都是他所在的簇中心本身
  - SSE变化过程中会出现一个拐点，下降率突然变缓即认为是最佳n_clusters的值
  - 再决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在增加分类无法带来更多回报时，我们停止增加类别

## 3.SC轮廓系数法

- 轮廓系数法考虑簇内的内聚程度，簇外的分离程度（至少两簇以上才能使用这个方法）
  - 对计算每一个样本$i$到同簇内其他样本的平均距离$a_i$，该值越小，说明簇内的相似程度越大
  - 计算每一个样本$i$到最近簇$j$内的所有样本的平均距离$b_{ij}$，该值越大，说明该样本越不属于其他簇
  - 公式：$S= \frac{b-a}{max(a, b)}$
  - 计算所有样本的平均轮廓系数
  - 轮廓系数的范围为$S \in [-1, 1]$，$SC$值越大，聚类效果越好

## 4.CH轮廓系数法

- CH系数考虑簇内的内聚程度，簇外的离散程度，质心的个数

- 类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好，聚类的**种类数越少越好**

- 公式：$CH(k) = \frac{SSB}{SSW} \frac{m-k}{k-1}$  

  $SSW = \sum_{i = 1}^m||x_i-C_{pi}||^2$  

  $SSB=\sum_{j = 1}{k}n_j||C_j-\overline X||^2$

  - $SSW$：相当于$SSE$，簇内距离
    - $C_{pi}$表示质心
    - $x_i$表示某个样本
    - $SSW$值是计算每个样本到质心的距离，并累加起来
    - $SSW$表示簇内的内聚程度，越小越好
  - $SSB$：簇间距离
    - $C_j$表示质心，$\overline X$表示质心与质心之间的中心点，$n_j$表示样本的个数
    - $SSB$表示簇与簇之间的分离程度，$SSB$越大越好
    - $m$表示样本数量