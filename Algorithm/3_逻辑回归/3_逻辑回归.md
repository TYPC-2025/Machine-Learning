# 一、逻辑回归简介

## 1.应用场景

> 逻辑回归是用于解决二分类问题

- 新冠预测: 根据个人行程等信息，预测是否感染新冠病毒。
- 商品评价: 在电商平台，根据用户购买和使用商品的情况，预测用户是否会给予好评。
- 广告点击: 根据用户画像，预测用户是否会点击弹出的广告。
- 商品推荐: 在推荐系统中，根据用户行为和偏好，预测用户是否对推荐商品感兴趣。
- 垃圾邮件过滤: 识别邮件是否为垃圾邮件。
- 人脸解锁/识别: 判断手机人脸解锁时是否为机主本人。
- 年龄判断: 如网吧入口的人脸检测，判断用户是否成年。

## 2.数学基础

### （1）sigmiod函数

- 公式：$f ( x ) = \frac { 1 } { 1 + e ^ { - x } }$﻿

- 作用 将$(0, +\infty)$﻿映射到$(0,1)$区间。
- 性质: 单调递增函数，拐点在x=0, y=0.5的位置。
- 导函数：$f ' ( x ) = f ( x ) ( 1 - f ( x ) )$﻿

- 阈值设定: 阈值的设定直接影响决策结果，通常设置阈值为0.5，根据实际需求可调整。若输出概率大于阈值，则认为事件发生；反之，则认为事件不发生。
- 函数可导，则可通过梯度下降算法来更新weight,来求解逻辑回归的结果

### （2）极大似然估计

- 核心思想：根据观察到的结果来估计模型算法中的未知参数

- 示例：假设有一枚不均匀的硬币，正面出现概率为θ，抛掷6次得到现象D={正面，反面，反面，正面，正面，正面}，通过极大似然估计求θ。

  - 似然函数:

    ﻿$f(\theta) = \theta^4(1 - \theta)^2$﻿

  - 求导求极值（问题已经转化成：此似然函数的极大只时，估计$\theta$为多少）:

    ﻿$\frac{\partial f(\theta)}{\partial \theta} = \theta^3(1 - \theta)(4 - 6\theta) = 0$﻿，解得$\theta_1 = 0, \theta_2 = 1, \theta_3 = \frac{2}{3}$，取$\theta_3 = \frac{2}{3}$为估计值（出现正面的概率）。

- 极大似然函数先通过对数函数将乘法转化成加法，再前面加上一个负号，就是求整个函数（此时可以视为损失函数）的最小值，就可以通过梯度下降算法来求解

### （3）概率

- 定义: 概率是一个事情发生的可能性，取值范围在零到一之间。
- 特性: 概率越接近于一，说明事情发生的可能性越大；越接近零，说明事情不太可能发生

#### A、联合概率

- 定义: 联合概率是两个或多个事情同时发生的概率。
- 独立事件: 如果两个事件是独立的，那么这两个事件同时发生的联合概率就是各自概率的乘积。

#### B、条件概率

- 定义: 条件概率是在另一个事件发生的情况下，某一事件发生的概率。
- 表示: 条件概率表示为﻿$P(A|B)$，即在B发生的情况下A发生的概率。
- 计算: 联合概率$P(A,B)$﻿可以表示为$P(A|B) \times P(B)$﻿

# 二、逻辑回归原理

## 1.基础

- 定义： 逻辑回归是一种分类模型，用于解决二分类任务。它将线性回归的输出作为输入，通过sigmoid函数将输出值映射为(0,1)之间的概率值。

- 基本思想：
  - 首先，利用线性模型$f(x) = w^Tx + b$﻿根据特征的重要性计算出一个值；
  - 然后，使用sigmoid函数将$f(x)$﻿的输出值映射为概率值。
    - 设置阈值（0.5），输出概率值大于0.5，则将未知样本输出为1类
    - 否则输出为0类
    - 上面的0，1也可以看作概率值，作为我们训练时的目标值

- 假设函数：$h_(w)=sigmiod(w^Tx+b)$

## 2.损失函数

（对数自然损失函数）

- 公式：﻿$\operatorname{Loss}(L) = -\sum_{i=1}^{m}\left(y_{i}\log\left(p_{i}\right) + \left(1-y_{i}\right)\log\left(1-p_{i}\right)\right)$，其中$p_i=\frac{1}{1-e^{-(w^T+b)}}$
  - 正样本处理
    - 当$y_i=1$﻿时，保留$y_i\log(p_i)$﻿项
    - 要使损失最小，需使$p_i$﻿（预测值）接近1
  - 负样本处理
    - 当$y_i=0$﻿时，保留$(1-y_i)\log(1-p_i)$﻿项
    - 要使损失最小，需使$p_i$接近0

- 优化目标
  - 训练目标: 通过梯度下降算法最小化损失函数，可更新逻辑回归算法中的权重参数
  - 数学原理
    - 当预测概率与真实标签一致时损失值最小
    - 利用对数性质将概率乘积转化为可加形式
  - 实现要点
    - 正样本要推动预测概率向1靠近
    - 负样本要推动预测概率向0靠近
    - 最终使整体损失函数值最小化

- 损失函数推导

  - 概率表示

    - 正样本出现概率（y=1）：$p$﻿
    - 负样本出现概率（y=0）：$1−p$﻿

    - 统一表达式：单个样本概率可表示为$p^y(1-p)^{1-y}$﻿，当

      ﻿$y=1$﻿时简化为$p$﻿，$y=0$时简化为$1−p$﻿

    - 训练集概率：将n个样本的概率连乘得到极大似然函数

      ﻿$L=\prod_{i=1}^n p_i^{y_i}(1-p_i)^{1-y_i}$

  - 对数转换过程

    - 转换原因：连乘运算复杂且可能下溢，转换为对数形式便于计算

    - 对数似然函数

      ﻿$\log L = \sum_{i=1}^n [y_i\log p_i + (1-y_i)\log(1-p_i)]$

    - 损失函数转换：为求最小值，在对数似然前加负号得到最终损失函数形式

# 三、分类问题评估

## 1.混淆矩阵（confusion_matrix）

|      |   正例   |   假例   |
| :--: | :------: | :------: |
| 正例 | 真正例TP | 伪反例FN |
| 假例 | 伪正例FP | 真反例TN |

## 2.精确率

- 定义：预测为正例样本中实际为正例的比例
- 公式：$Precision=\frac{TP}{TP+FP}$

## 3.召回率

- 定义：实际为正例的样本中被正确预测的比例
- 公式：$Recall=\frac{TP}{TP+FN}$

## 4.F1-score

- 若对模型的精度，召回率都有要求，希望知道模型在这两个评估方向的综合预测能力，则会使用F1-score
- 公式：$P=\frac{2*Precision*Recall}{Precision+Recall}$

## 5.ROC曲线 AUC指标

- 基本概念

  - ROC曲线通过不同阈值下的TPR（真正例率）和FPR（假正例率）绘制而成

  - TPR计算公式：

    $TPR = \frac{TP}{TP+FN}$﻿，表示正样本中被正确预测的比例

  - FPR计算公式：

    ﻿$FPR = \frac{FP}{FP+TN}$，表示负样本中被错误预测的比例

- AUC指标（是ROC曲线的线下面积）：AUC越大表示分类器性能越好

  - AUC=0.5，表示分类器的性能等同于随机猜测
  - AUC=1，表示分类器的性能完美，能够完全正确地将正反例分类

- 绘制步骤

  - 将样本按预测概率从高到低排序
  - 依次以每个样本的预测概率作为阈值
  - 计算每个阈值对应的TPR和FPR
  - 在坐标系中绘制所有(TPR,FPR)点并连接成曲线

- 特殊点说明 （TPR，FPR）
  - （0，0）：所有负样本都预测正确，所有正样本都预测错误
  - （1，0）：所有负样本都预测错误，所有正样本都预测错误
  - （1，1）：所有负样本都预测错误，所有正样本都预测正确
  - （0，1）：所有负样本都预测正确，所有正样本都预测正确

- AUC的计算

  ```python
  from sklearn.metrics import roc_auc_score
  sklearn.metrics.roc_auc_score(y_true,y_score)
  ```

## 6.返回所有评估指标

```python
sklearn.metrics.classification_report(y_true,y_pred,labels=[],target_names=None)
```

- y_true：真实目标值
- y_pred：估计器预测的目标值

- labels：指定类别对应的数字
- target_names：目标类别名称
- return 每个类别精确率和召回率