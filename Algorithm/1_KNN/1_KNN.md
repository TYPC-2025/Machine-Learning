# 一、KNNN算法简介

## 1.KNN思想

### （1）K-近邻算法

- 根据你的“邻居”来推断你是什么类别

- KNN算法思想：如果一个样本在特征空间（训练集）中的k个最相似的样本中的大多数属于某一个类别。则该样本也属于这个类别

### （2）样本相似性

- 样本都是属于一个任务数据集的，样本距离越近则越相似

- 计算样本距离
  - 欧氏距离：$\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$

### （3）分类问题处理流程

- 距离计算: 使用欧氏距离计算未知样本到每个训练样本的距离
- 排序规则: 将所有训练样本按距离从小到大排序
- K值选择: 选取距离最近的K个样本（K值由程序员预先设定）
- 表决机制: 统计K个样本中多数类别，将该类别作为未知样本的预测结果

### （4）回归问题处理流程

- 距离计算: 使用欧氏距离计算未知样本到每个训练样本的距离
- 排序规则: 将所有训练样本按距离从小到大排序
- K值选择: 选取距离最近的K个样本（K值由程序员预先设定）
- 表决机制: 对K个样本的标签值（目标值）取平均值，作为未知样本预测的值

### （5）K值设置

#### a. K值过小

- 异常值敏感：当K=1时，若最近邻是异常值，预测结果会完全错误
- 模型复杂度：K值减小会使模型变复杂，容易发生过拟合
- 学习偏差：会学到训练集中不该学的噪声特征，如当K=1时可能错误学习异常点的特征

#### b. K值过大

- 样本均衡问题：当K=N（训练样本总数）时，预测结果总是训练集中最多的类别
- 模型简化：K值增大会使模型变得过于简单，导致欠拟合

#### c. K值选择的经验法则 

- 二分类问题：避免选择2的倍数（如2、4等）
- 三分类问题：避免选择3的倍数（如3、6等）
- 五分类问题：避免选择5的倍数

#### d. K值调优方法

交叉验证网格搜索 

- 通用方法：适用于所有算法的超参数调优
- 实现方式：通过交叉验证评估不同K值表现，网格搜索寻找最优参数
- 注意事项：调优过程需考虑计算成本与模型性能的平衡

# 二、KNN算法API

## 1.KNN分类API

```python
# 1.工具包
from sklearn.neighbors import KNeighborsClassifier

# 2.数据（特征处理）
x = [[0],[1],[2],[3],[4]]
y = [0,0,1,1,1]

# 3.实例化
model = KNeighborsClassifier(n_neighbors=3) # 二分类，避免设置成2的倍数

# 4.训练
model.fit(x,y)

# 5.预测
print(model.predict([[5]])) # 输出[1]
```

- 多维特征处理
  - 特征可以是任意维度，如三维特征:$﻿X=[[0,2,3],[1,3,4],[3,5,6]]$
  - 预测时输入特征维度必须与训练数据一致

- 变量命名:
  - 模型实例可命名为estimator或model
  - 预测结果通常保存到变量如myret
- 注意事项
  - 分类和回归使用不同API，不能混用
  - 输入数据必须是二维数组格式，即使单个样本也要用**双层括号**
  - 特征工程可以插入在数据准备和模型训练之间
  - 预测结果打印使用print函数查看

## 2.KNN回归API

```python
# 1.工具包
from sklearn.neighbors import KNeighborsRegressor

# 2.数据（特征工程）
x = [[0],[1],[2],[3]]
y = [0.1,0.2,0.3,0.4]  # 目标值是连续的

# 3.实例化
model = KNeighborRegressor(n_neighbors=3)

# 4.训练
model.fit(x,y)

# 5.预测
print(model.predict([[5]])) # 输出[0.3]
```

# 三、距离度量

## 1.欧氏距离

$\sqrt{\sum_{i=1}^n(x_i-y_i)^2}$

## 2.曼哈顿距离

- 别称：城市街区距离(City Block distance)
- 几何意义：在横平竖直的街区道路中，从一个点到另一个点需要行走的最短路径长度
- 二维公式：点﻿$a(x1,y1)$与$b(x2,y2)$间距离为$d_{12}=|x_1-x_2|+|y_1-y_2|$
- n维公式：$d=\sum^n|x_i-x_j|$
- 计算原理：对应坐标相减取绝对值后求和

## 3.切比雪夫距离

- 计算公式: 二维平面两点$a(x_1,y_1)$与$b(x_2,y_2)$间的切比雪夫距离为

  $d_{12} = \max(|x_1 - x_2|, |y_1 - y_2|)$

- 移动特性: 相比只能沿$xy$方向走的曼哈顿距离，切比雪夫距离允许沿45度对角线方向移动，这是其核心改变。

## 4.闵可夫斯基距离

- - 统一形式：可将欧氏距离和曼哈顿距离统一为闵可夫斯基距离

    $d_{12} = \sqrt[p]{\sum_{k=1}^n |x_{1k}-x_{2k}|^p}$

  - 参数关系：

    - 当$p=1$时为曼哈顿距离
    - 当$p=2$时为欧氏距离
    - 当$p\to\infty$时为切比雪夫距离

## 5.其他距离

余弦距离、马式距离（不通用）

# 四、特征预处理

## 1.原因

当特征的单位或大小相差较大，或某特征的方差比其他特征大出几个数量级时，会影响（支配）目标结果，使模型无法有效学习其他特征。

## 2.归一化

- 定义：将原始数据通过线性变换映射到指定范围（默认[0,1]）的方法。

- 基本公式：$⁡X^{\prime} = \frac{x - \min}{\max - \min}$

- 扩展公式：若需映射到$[mi,mx]$范围，则

  $X^{\prime\prime} = X^{\prime} * (mx - mi) + mi$

- 特点与适用场景

  - 异常值敏感：受最大值最小值影响大，若存在异常值（如年龄特征出现200岁）会显著影响归一化结果。
  - 适用场景：适合取值范围固定且无异常值的数据（如图像像素值固定为0-255）。
  - 不适用场景：不适合大规模数据或存在异常值的情况。

- API

```python
# 1.导入工具包
from sklearn.preprocessing import MinMaxScaler

# 2.数据（只有特征）
x = [[90,2,10,40],[60,4,15,45],[75,3,13,46]]

# 3.实例化
process = MinMaxScaler()

# 4.fit_transform处理
data = process.fit_transform(x)
```

## 3.标准化

- 通过对原始数据进行标准化，转换为均值为0，标准差为1的标准正态分布的数据
- 公式：$x^{\prime}=\frac{x-mean}{\sigma}$（$\sigma为特征的标准差$）
- 少量的异常值对于平均值的形象并不大，鲁棒性更强，优先选择

- API

```python
# 1.导入工具包
from sklearn.preprocessing import StandardScaler

# 2.数据（只有特征）
x = [[90,2,10,40],[60,4,15,45],[75,3,13,46]]

# 3.实例化
process = StandardScaler()

# 4.fit_transform处理
data = process.fit_transform(x)

print(data)
print(process.mean_)
print(process.var_)
```

# 五、超参数选择

## 1.交叉验证

- 核心思想: 将训练集划分为n份，每次取1份作为验证集，其余n-1份作为训练集，循环n次
- 验证集作用: 与测试集功能相同，用于评估模型效果
- 折数命名: 根据划分份数称为n折交叉验证（如4份即四折交叉验证）
- 评估方式: 取多次验证结果的**平均值**作为最终模型评分，比单次划分更可靠

## 2.网格搜索

- 产生背景: 模型存在多个需人工设置的超参数（如KNN的k值），不同参数组合效果差异大
- 工作原理:
  - 预设参数候选值（如k=3,4,5,6）
  - 对每个参数组合进行交叉验证评估
  - 自动选择最优参数组合（如k=5时准确率86%最高）
- 优势特点: 避免手动调参的低效，自动完成参数组合、训练、评估全流程

## 3.联系

- 关键区别:
  - 交叉验证是数据集划分方法
  - 网格搜索是参数优化工具
- 最佳实践: 两者结合使用可形成完整的模型调优方案
- 注意事项: 仅对需要人工设置的超参数使用该方法，模型自动学习的参数不适用