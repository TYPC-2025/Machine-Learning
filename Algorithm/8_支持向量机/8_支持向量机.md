# 一、支持向量机

- 支持向量机只能做二分类任务

- SVM全称支持向量机，即寻找到一个超平面使样本分成两类，且间隔最大

- 硬间隔：如果样本线性可分，在所有样本分类都正确的情况下，寻找最大间隔；如果出现异常值或样本线性不可分，此时硬间隔无法实现
- 软间隔：允许部分样本，在最大间隔之内，甚至在错误的一边，寻找最大间隔；目标是尽可能保持间隔宽阔和限制间隔违例之间寻找良好的平衡
- 惩罚系数：通过惩罚系数来控制这个平衡，C值越小，则间隔越宽，分错的样本个数也就越多；反之，C值越大，则间隔越窄，分错的样本个数越少

# 二、LinearSVC_API

```python
class sklearn.svm LinearSVC(C = 1.0)
```

- 示例

```python
from plot_util import plot_decision_boundary_svc, plot_decision_boundary
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt
from sklearn.svm import LinearSVC

X, y = load_iris(return_X_y= True)

x = X[y < 2, :2]
y = y[y < 2]

plt.scatter(x[y == 0, 0], x[y == 0, 1], c = 'r')
plt.scatter(x[y == 1, 0], x[y == 1, 1], c = 'b')
plt.show()

# 特征处理
transform = StandardScaler()
x_tran = transform.fit_transform(x)

# 模型训练
model = LinearSVC(C = 30)
model.fit(x_tran, y)
y_pre = model.predict(x_tran)
print(accuracy_score(y, y_pre))

# 可视化处理
plot_decision_boundary_svc(model, axis = [-3, 3, -3, 3])
plt.scatter(x_tran[y == 0, 0], x_tran[y == 0, 1], c = 'r')
plt.scatter(x_tran[y == 1, 0], x_tran[y == 1, 1], c = 'b')
plt.show()

# 模型训练
model = LinearSVC(C = 0.01)
model.fit(x_tran, y)
y_pre = model.predict(x_tran)
print(accuracy_score(y, y_pre))

# 可视化处理
plot_decision_boundary_svc(model, axis = [-3, 3, -3, 3])
plt.scatter(x_tran[y == 0, 0], x_tran[y == 0, 1], c = 'r')
plt.scatter(x_tran[y == 1, 0], x_tran[y == 1, 1], c = 'b')
plt.show()
```

# 三、SVM算法原理

要去求一组参数（w, b），使其构建的超平面函数能够最优地分离两个集合

样本空间中任一点x到超平面（w, b）的距离可写成：$r = \frac{w^Tx+b}{||w||}$，想要找到具有最大间隔的划分超平面，也就是要找到能满足下式中约束的参数w和b，使得间隔$\gamma$最大

​                                     $ \begin{cases} \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i} + b \geqslant +1, & y_{i} = +1; \\ \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i} + b \leqslant -1, & y_{i} = -1. \end{cases} $

距离超平面最近的几个训练样本点使上式等号成立，他们被称为“支持向量”，两个异类支持向量到超平面的距离之和为（最大间隔距离表示）：$\frac{2}{||w||}$

- 训练样本：$ \begin{cases} \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i} + b \geqslant +1, & y_{i} = +1; \\ \boldsymbol{w}^{\mathrm{T}} \boldsymbol{x}_{i} + b \leqslant -1, & y_{i} = -1. \end{cases} $则目标函数可以写成：$max_{w, b} = \frac{2}{||w||}s.t.y_i(w^Tx_i+b) \geqslant 1，其中i=1,2,3,\dots, m$
- 将目标函数进一步优化：$min_{w, b} = \frac{1}{2}||w||^2 s.t.y_i(w^Tx_i+b) \geqslant 1,其中i = 1, 2, 3, \dots, m$

- 添加核函数，将目标函数转化成以下形式：$min_{w, b} = \frac{1}{2}||w||^2 \begin{align*}&\text{s.t.}
   \\
  &\sum_{i = 1}^{n} \left(1 - y_{i} \left(\boldsymbol{w}^{\mathrm{T}} \cdot \boldsymbol{\varPhi}(x_{i}) + b\right)\right) \leqslant 0
  \end{align*}$

- 构建拉格朗日函数：其中$\alpha_i$为拉格朗日乘子（相当于$\lambda_i$）:$L(w, b, \alpha) = \frac{1}{2}||w||^2-\sum_{i = 1}^{n} \alpha_i\left(1 - y_{i} \left(\boldsymbol{w}^{\mathrm{T}} \cdot \boldsymbol{\varPhi}(x_{i}) + b\right)-1\right)  \dots \dots ① $

- 要想求得极小值，上式后半部分应该取的极大值：$min_{w, b}max_{\alpha}L(w, b, \alpha) <==> max_{\alpha }min_{w, b}L(w, b, \alpha)$

- 要找$min_{w, b}L(w, b, \alpha)$，既要先对$w, b$求导

  - 对$w$求偏导，并令其为0：$L=\frac{1}{2}||w||^2-\sum_{i = 1}^n \alpha_i(y_iw^T \varphi(x_i)+y_ib-1)=\frac{1}{2}||w||^2-\sum_{i = 1}^n \alpha_iy_iw^T \varphi(x_i)+\alpha_iy_ib-\alpha_i$

    $\frac{\partial L}{\partial w}= w-\sum_{i = 1}^n\alpha_iy_i \varphi(x_i) = 0$

    得到：$w =\sum_{i = 1}^n\alpha_iy_i \varphi(x_i) = 0$

  - 对b求偏导，并令其为0：
    $L = \frac{1}{2}||w||^2-\sum_{i = 1}^n \alpha_iy_iw^T\varphi(x_i)+\alpha_iy_ib-\alpha_i$

    $\frac{\partial L}{\partial b}=\sum_{i = 1}^n\alpha_iy_i=0$
    得到：$\sum_{i = 1}^n\alpha _iy_i = 0$

- 将上面两个求导的结果代入①式中，得到：$\begin{align*} L(\boldsymbol{w}, b, \boldsymbol{\alpha})&=\frac{1}{2}\|\boldsymbol{w}\|^{2}-\sum_{i = 1}^{n} \alpha_{i}(y_{i}(\boldsymbol{w}^{\mathrm{T}} \boldsymbol{\varphi}(x_{i}) + b) - 1)\\ &=\frac{1}{2}\boldsymbol{w}^{\mathrm{T}}\boldsymbol{w}-\sum_{i = 1}^{n}(\alpha_{i}y_{i}\boldsymbol{w}^{\mathrm{T}}\boldsymbol{\varphi}(x_{i})+\alpha_{i}y_{i}b - \alpha_{i})\\ &=\frac{1}{2}\boldsymbol{w}^{\mathrm{T}}\boldsymbol{w}-\sum_{i = 1}^{n}\alpha_{i}y_{i}\boldsymbol{w}^{\mathrm{T}}\boldsymbol{\varphi}(x_{i})-b\sum_{i = 1}^{n}\alpha_{i}y_{i}+\sum_{i = 1}^{n}\alpha_{i}\\ &=\frac{1}{2}\boldsymbol{w}^{\mathrm{T}}\boldsymbol{w}-\sum_{i = 1}^{n}\alpha_{i}y_{i}\boldsymbol{w}^{\mathrm{T}}\boldsymbol{\varphi}(x_{i})+\sum_{i = 1}^{n}\alpha_{i}\\ &=\frac{1}{2}\boldsymbol{w}^{\mathrm{T}}\sum_{i = 1}^{n}\alpha_{i}y_{i}\boldsymbol{\varphi}(x_{i})-\boldsymbol{w}^{\mathrm{T}}\sum_{i = 1}^{n}\alpha_{i}y_{i}\boldsymbol{\varphi}(x_{i})+\sum_{i = 1}^{n}\alpha_{i}\\ &=\sum_{i = 1}^{n}\alpha_{i}-\frac{1}{2}\left(\sum_{i = 1}^{n}\alpha_{i}y_{i}\boldsymbol{\varphi}(x_{i})\right)^{\mathrm{T}}\cdot\sum_{i = 1}^{n}\alpha_{i}y_{i}\boldsymbol{\varphi}(x_{i})\\ &=\sum_{i = 1}^{n}\alpha_{i}-\sum_{i = 1}^{n}\sum_{j = 1}^{n}\alpha_{i}\alpha_{j}y_{i}y_{j}\boldsymbol{\varphi}^{\mathrm{T}}(x_{i})\boldsymbol{\varphi}(x_{j}) \end{align*} $

#  四、SVM核函数

- 核函数作用：核函数将原始输入空间映射到新的特征空间，从而使原本线性不可分的样本可能在核空间可分
- 核函数分类
  - 线性核：$k(x_i, x_j)=x_i^Tx_j$
  - 多项式核：$k(x_i, x_j) = (x_i^Tx_j)^d$
  - 高斯核（RBF， 径向基函数）：$k(x_I, x_j) = epx(-\frac{||x_i-x_j||^2}{2\sigma^2})$——产生将样本投射到无限维空间的运算效果，使得原来不可分的数据变得可分，使用最多
  - 拉普拉斯核：$k(x_i, x_j)=exp(-\frac{||x_i-x_j||^2}{\sigma})$
  - Sigmod核：$k(x_i, x_j) = tanh(\beta x_i^Tx_j+\theta)$

## 1.高斯核

- 公式：$K(x, y) = e^{-\gamma||x-y||^2}$，其中$\gamma=\frac{1}{2\sigma^2}$
  - $\gamma$是超参数，作用与标准差相反，$\gamma$越大（标准差越小），高斯分布越窄，$\gamma$越小，高斯分布越宽

- API（$\gamma$较大，过拟合；$\gamma$较小，欠拟合）

```python
from sklearn.svm import SVC
SVC(kernel = 'rbf', gamma=gamma)
```

