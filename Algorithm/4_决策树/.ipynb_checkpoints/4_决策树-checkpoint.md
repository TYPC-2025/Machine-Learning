# 一、决策树的简介

- 核心思想: 与线性回归、逻辑回归不同，决策树将特征视为一系列条件，通过数据学习这些条件划分方式
- 特征处理: 特征作为分支条件，划分规则完全从数据中自动学习得到

- 决策树是一种树形结构：树中每个内部节点表示一个特征上的判断，每个分支表示一个判断结果，每个叶子节点代表一种分类结果
- 决策树的建立过程
  - 特征选择：选取有较强分类能力的特征
  - 决策树生成：根据选择的特征生成决策树
  - 决策树也易过拟合，采用剪枝的方法（一般是从深层开始剪枝）缓解过拟合

# 二、ID3决策树

混乱程度越大，则熵值越大；包含信息比较多，则比较混乱，则熵值越大

## 1.信息熵

- 熵（Entropy）:信息论中代表随机变量不确定度的度量
  - 熵越大，数据的不确定性度越高，信息就越多
  - 熵越小，数据的不确定性越低

- 从有序到无序的过程，就是熵增的过程
- 计算方法：$H(x)=-\sum_{i=0}^nP(x_i)\log_2P(x_i)$，其中$P(x_i)$表示数据中类别出现的概率，$H(x)$表示信息的信息熵值

- 我们要尽可能地使信息熵减少得越多越好

## 2.信息增益

- 定义：特征a对训练数据集D的信息增益$g(D,a)$，定义为集合$D$的熵$H(D)$与特征a给定条件下$D$的熵$H(D|a)$之差
- 公式：$g(D, A)=H(D)-H(D|A)$（信息增益=熵-条件熵）

- 条件熵（即在熵值前面乘以条件所占比例）：$H(D|A)=\sum_{v=1}^n\frac{D^V}{D}H(D^v)=\sum_{v=1}^n\frac{D^V}{D}\sum_{k=1}^{kV}\log\frac{c^{kV}}{D^V}$

- 最终求得信息增益之后，找信息增益最大的结果作为分支

## 3.ID3决策树构建流程

- 计算每个特征的信息增益
- 使用信息增益最大的特征将数据集拆分为子集
- 使用该特征（信息增益最大的特征）作为决策树的一个节点
- 使用剩余特征对子集重复上述（1，2，3）过程

## 4.ID3决策树的缺点

- 对类别较多的特征比较青睐

# 三、C4.5决策树

## 1.信息增益率

- $信息增益率=\frac{信息增益}{特征熵}$
- 计算方法：$Gain\_Ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$
  - $Gain\_Ratio(D,a)$  信息增益率
  - $IV(a)=-\sum_{v=1}^n\frac{D^V}{D}Ent(\frac{D^V}{D})$
- 信息增益率的本质
  - 特征的信息增益/特征的内在信息
  - 相当于对信息增益进行修正，增加了一个惩罚系数
  - 特征取值个数较多时，惩罚系数较小；特征取值个数较少时，惩罚系数较大
  - 惩罚系数：数据集D以特征a作为随机变量的熵的倒数

# 四、CART分类树

- 回归任务中数据是连续的，而前面两种树不适合用于连续型数据，故要做回归任务，应该是选择CART树
- CART既可以用于分类，也可以用于回归
- CART回归树使用**平方误差最小化**策略
- CART分类生成树采用的**基尼指数最小化**策略

## 1.Gini指数

- 基尼值Gini（D）：从数据集D中随机抽两个样本，其类别标记不一致的概率，故Gini（D）值越小，数据集D的纯度越高

  $Gini(D)=\sum_{k=1}^{|y|}\sum_{k'\neq k}p_kp_{k'}=1-\sum_{k=1}^{|y|}p_k^2$

- 基尼指数Gini_index(D)：选择使划分后基尼系数最小的属性作为最优划分属性

  $Gini\_index(D,a)=\sum_{v=1}^V\frac{D^v}{D}Gini(D^v)$

- 注意：基尼指数值越小，则说明优先选择该特征

- 在有多个类别时，应该划分为多个二类分来求Gini指数

## 2.当特征取值是连续的时候，应该怎样找到划分节点？

- 先将数值型属性升序排列，以相邻中间值作为待确定分裂点

- 以中间值将样本分为两部分，计算基尼指数
- 以此类推计算所有分割点的基尼指数，以最小基尼指数对应的值来进行划分