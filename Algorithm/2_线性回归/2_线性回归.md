# 一、线性回归简介

## 1.基础

- 定义：利用回归方程（函数）对一个或多个自变量（特征值）和因变量（目标值）之间关系进行建模的一种分析方式

- 数学公式：$h_{(w)}=w_1x_1+w_2x_2+w_3x_3+\dots+b=w^Tx+b$，其中$w$为$ \begin{pmatrix} b \\  w_1 \\ w_2 \\ \vdots \end{pmatrix}  $，$x$为$ \begin{pmatrix} 1 \\  x_1 \\ x_2 \\ \vdots \end{pmatrix}  $，根据矩阵运算：$w^Tx$为$(b, w_1,w_2,\dots)@ \begin{pmatrix} 1 \\  x_1 \\ x_2 \\ \vdots \end{pmatrix} $
  - $w$：权重向量，包含偏置项$b$和各特征权重
  - $x$：特征向量，首项为1对应偏置项

- 线性特性：之所以称为线性模型，因为所有参数$w$都是零次幂（常数项）

- 线性回归特征影响
  - 权重解释：某个权重值$w_i$越大，说明对应特征$x_i$对目标值影响越大
  - 特征选择依据：可根据权重值大小进行特征筛选，保留权重较大的特征

# 二、线性回归问题的求解

> 导入线性回归包-->准备数据-->实例化线性回归模型-->训练线性回归模型--> 模型预测

## 1.API

```python
from sklearn.linear_model import LinearRegression

# 数据
x = [[160],[165],[172],[174],[180]]
y = [56.3, 60.6, 65.1,68.5, 75]

# 模型训练
model = LinearRegression()
model.fit(x, y)

# 权重偏置
print(model.coef_)
print(model.intercept_)

# 预测
model.predict([[176]])
```

## 2.损失函数

### （1）误差

- 定义：误差是指预测值$﻿y$﻿与真实值$y$﻿之间的差异，计算公式为：

  ﻿误差=预测值$y$−真实值$y$

- 正负处理：由于误差有正有负，实际计算时需要取**绝对值**或**平方**来消除方向性影响

### （2）损失函数

#### A、基础

- 别名：代价函数，成本函数，目标函数

- 核心作用：衡量所有样本预测值与真实值之间的总体误差
- 计算范围：必须包含训练集中每个样本的误差，而非部分样本
- 最优标准：最优拟合线就是使损失函数值最小的那条线

使损失函数最小的即为优化方法，包括正规方程法和梯度下降算法

#### B、损失函数种类

- 最小二乘损失函数（L2范数求平方）：$损失函数J(w,b)=\sum_{i=0}^m(h(x^{i})-y^{(i)})^2=||Xw-y||^2$

- 均方误差（Mean-Square Error,MSE）:$损失函数J(w,b)=\frac{1}{m}\sum_{i=0}^m(h(x^{i})-y^{(i)})^2$

- 平均绝对误差（Mean Absolute Error,MAE）:$损失函数J(w,b)=\frac{1}{m}\sum_{0}{m}|h(x^{(i)}-y^{(i)}|$

> 数据-->线性回归模型-->损失函数-->优化方法

## 3.优化方法

找损失函数最小时对应的$w$和$b$值

### （1）正规方程法

- API

  ```python
  sklearn.linear_model.LinearRegression(fit_intercept=True)
  ```

  - 通过正规方程优化
  - 参数：`fit_intercept`，是否计算偏置
  - 属性：`LinearRegression.coef_`（回归系数），`LinearRegression.intercept_`（偏置）直接求导数为0的点，就是损失函数最小值

- $损失函数J(w,b)=\sum_{i=1}^m(h(x^{i})-y^{(i)})^2=||Xw-y||^2=\sum_{i=1}^m(kx^{(i)}+b-y^{(i)})^2$
- 对$k$,$b$分别求偏导，找偏导为0时的$k$,$b$，此时损失函数值最小
- $w=(X^TX)^{-1}*X^Ty$
  - 注意$X$是会在$x_1,x_2,\dots,x_n$这些列之前再加上一列$x_0$，该列内容全是1
- 只有小数据时的时候才能用正规方程法来做

### （2）梯度下降算法

沿着梯度下降方向求解极小值

- API

  ```python
  sklearn.linear_model.SGDRegressor(loss="squared_loss",fit_intercept=True,learning_rate="constant",eta0=0.01)
  ```

  - 参数
    - `loss`（损失函数类型）：`loss='squared_loss'`（默认MSE）
    - `fit_intercept`（是否计算偏置）
    - `learning_rate`（学习率策略）：`constant(保持恒定学习率)，string，optional，`可以配置学习率随着迭代次数不断减小
      - 学习率退火策略：学习率随迭代次数增加而逐渐减小（初期：较大学习率---离最优解较远；后期：较小学习率---接近最优解
      - `‘invscaling’:eta=eta0/pow(t,power_t=0.25)`（t是迭代次数）
    - `eta0=0.01`（学习率的值）
  - 属性：`LinearRegression.coef_`（回归系数），`LinearRegression.intercept_`（偏置）直接求导数为0的点，就是损失函数最小值

#### A. 梯度

- 数学定义：
  - 单变量：某点切线斜率（导数），方向为函数增长最快方向
  - 多变量：偏导数向量，方向为各分量偏导数构成的方向
- 关键性质
  - 梯度方向是上升最快方向
  - 梯度下降需取负梯度方向
  - 计算方式：对损失函数求导后取负

#### B. 梯度下降过程

- 核心公式：$\theta_{i+1} = \theta_i - \alpha \frac{\partial}{\partial \theta_i}J(\theta)$
  - $\theta$：权重
- 参数说明
  - $\theta$：权重参数
  - $J(\theta)$：损失函数
  - ﻿$\alpha$：学习率（关键超参数），控制迭代过程中迈的步子的大小
    - 过大（如$\alpha=2$）：可能越过最优点
    - 过小：收敛速度慢
    - 经验值：0.001-0.01（神经网络更小）
- 执行逻辑：循环计算当前点梯度，按学习率更新参数

#### C. 单变量梯度下降

#### D. 多变量梯度下降

- 对每个变量分别求偏导构造梯度
- 梯度方向由各分量偏导数共同决定
- 负梯度方向即为函数下降最快方向

#### E. 梯度下降优化过程

- 实现步骤
  - 人工设定初始位置
  - 设置学习率α
  - 计算当前点梯度（偏导数）
  - 沿负梯度方向移动步长
  - 重复3-4步直到终止条件
- 终止条件
  - 常见方法：指定迭代次数（如1000次）
  - 可选方法：当步长小于设定阈值时停止
- 判断依据
  - 接近最优解时梯度较小
  - 步长变化量可反映接近程度

#### F. 学习率

- 学习率作用
  - 决定每次迭代沿负梯度方向前进的长度
  - 直接影响收敛速度和结果质量
- 学习率问题
  - 太小：收敛速度慢，需要大量迭代
  - 太大：可能错过最优解，产生震荡甚至梯度爆炸
- 学习率选择
  - 需要平衡收敛速度和稳定性
  - 典型值范围：0.001-0.01
  - 可通过实验调整确定最佳值

- 学习率对比:
  - 小学习率：稳定但缓慢
  - 大学习率：可能震荡
- 实际应用建议:
  - 初始可尝试中等大小学习率
  - 观察损失函数下降曲线调整
  - 出现震荡时应减小学习率

#### G. 梯度下降算法分类

##### a. 全梯度下降算法（FGD） 

- 定义: 每次迭代时使用全部样本来计算梯度值。
- 缺点: 计算量大，硬件支撑可能不足，训练速度慢。

##### b. 随机梯度下降算法（SGD）

- 定义: 每次迭代时只使用一个样本来计算梯度值并更新模型参数。
- 特点: 计算快，但容易受到噪声或异常值的影响，不稳定，可能陷入局部最优解，初期表现不佳

##### c.  小批量梯度下降算法（mini-batch）

- 定义: 每次迭代时，从m个样本中随机选择x个样本（$﻿1<x<m$）来计算梯度值并更新模型参数。
- 优点: 结合了SGD的快速和FGD的稳定性，运算效率较高，收敛效果较稳定。
- 实际应用: 是实际工作中使用最多的梯度下降算法。

##### d. 随机平均梯度下降算法（SAG）

- 定义: 在SGD的基础上，对每次迭代计算的梯度值进行平均，作为最终的梯度值来更新模型参数。
- 目的: 减小异常值对梯度更新的影响，提高算法的稳定性。

# 三、回归模型评估方法

在模型训练好之后又回来评估方法的时候使用；我们希望衡量预测值和真实值之间的差距，则会进行线性回归模型的评估

## 1.三种评估方法

- 平均绝对误差 Mean Absolute Error（MAE）

  - 计算公式

    $MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$

  - API调用

    ```python
    from sklearn.metrics import mean_absolute_error
    mean_absolute_error(y_test,y_predict)
    ```

  - 特点：对异常值不敏感，给出平均误差水平

- 均方误差Mean Squared Error（MSE）

  - 计算公式

    ﻿$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$

  - API调用

    ```python
    from sklearn.metrics import mean_squared_error
    ```

  - 特点：放大较大误差的影响

- 均方根误差Root Mean Squared Error（RMSE）

  - 计算公式

    ﻿$RMSE = \sqrt{MSE}$

  - 实现方式：需先计算MSE再手动开平方（无直接API）

  - 特点：量纲与原始数据一致，解释性更好

- 注意：上面三个值均是越小，效果越好

## 2.MAE、MAE、RMSE三种指标对比

- 数值关系：通常RMSE > MAE（因平方放大效应）
- 敏感度差异
  - RMSE对异常值更敏感
  - MAE对所有误差平等对待
- 过拟合提示：RMSE过小可能提示模型过度拟合噪声
  - 解释：RMSE减小，则模型对距离比较大的异常点的拟合效果变好，则模型学到了不该学的东西
- 使用建议
  - 同一项目保持评估指标一致
  - 可根据业务需求选择敏感度不同的指标
  - 需结合其他指标综合判断模型表现

- 实践建议
  - 一般优先使用MAE和RMSE
  - RMSE值异常降低时要警惕过拟合
  - 评估时需考虑指标的业务解释性

# 四、欠拟合和过拟合

工作中训练模型，先训练到过拟合，再进行正则化

## 1.出现原因

- 欠拟合

  - 核心特征：模型在训练集和测试集上表现都不好

  - 模型状态：模型过于简单，无法捕捉数据特征
  - 误差表现：训练误差和测试误差都较大（如均方误差3.075

- 误差对比

  - 欠拟合：训练误差≈测试误差且都大
  - 过拟合：训练误差小但测试误差大

  - 泛化能力：欠拟合时模型未学到有效特征，过拟合时泛化能力差

- 示例：当线性方程拟合效果不好时，可以增加模型复杂度（如改用多项式回归）和添加高阶特征项从而加强拟合效果

## 2.解决办法

### （1）欠拟合

- 特征工程

  - 添加新特征：通过组合、泛化或相关性分析增加特征（如线性模型添加$x^2$、$x^3$项）
  - 多项式特征：将线性模型扩展为多项式模型增强泛化能力

  - 模型升级：更换为容量更大的复杂模型（如从线性回归升级为神经网络）

### （2）过拟合（重要）

- 数据处理
  - 清洗数据：移除明显异常值（如年龄200岁的记录）
  - 增加数据量：扩大训练集规模降低异常值影响比例
- 正则化：解决模型过拟合的方法
- 其他技术
  - 特征选择：通过分析权重大小剔除不重要特征
  - 早停法：在验证集性能下降时提前终止训练
  - 批标准化：缓解内部协变量偏移问题
  - 减少特征维度，防止维灾难：由于特征多，样本数量少，导致学习不充分，泛化能力差

### （3）正则化

- 概念：在模型训练时，数据中有些特征影响模型复杂度，或者耨个特征的异常值较多，所以要尽量减少这个特征的影响（甚至删除某个特征的影响），也就是使相应的权重尽可能趋近于0

- 正则化如何消除异常点带来的w值过大过小的影响：再损失函数中增加正则化项，分为L1正则化，L2正则化

#### A、L1正则化

会直接把高次项前面的系数变为0

- 定义：在损失函数中添加L1范数作为正则化项，公式为

  $J(w) = MSE(w) + \alpha \sum_{i=1}^{n}|w_i|$

- 惩罚系数

  - $\alpha$控制正则化强度，值越大惩罚力度越大
  - 属于需要人工调整的超参数

- 权重影响

  - 通过绝对值函数的梯度特性（x>0导数为1，x<0导数为-1）迫使权重趋向0
  - 可能使不重要的特征权重精确等于0，实现特征筛选

- 应用场景
  - 适用于需要特征选择的场景
  - 加入线性回归后形成Lasso回归模型
  
- 优化过程
  - 初始化权重后，正则化项的负梯度方向会持续推动权重向0靠近
  - 当权重=0时梯度消失，优化停止
  
- 使用L1正则化的线性回归模型就是Lasso回归

#### B、L2正则化（优先选择）

- 公式：$J(w)=MSE(w)+\alpha\sum_{i=1}^nw_i^2$
  - $\alpha$叫做惩罚系数，该值越大则权重调整幅度就越大，即表示对特征权重惩罚力度就越大
- L2正则化会使得权重趋向于0，一般不等于0

- 使用L2正则化的线性回归模型是**岭回归**